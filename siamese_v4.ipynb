{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese_v4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMSb/b3/ijyXVJdbFn1P0JR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChirathD/Notebooks/blob/master/siamese_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9RaM8UnBN5M"
      },
      "source": [
        "import re\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3-LXrmVYNdv"
      },
      "source": [
        "from os import walk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow \n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrtZWv_KxGnC"
      },
      "source": [
        "!gdown --id 0B8-rUzbwVRk0c054eEozWG9COHM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olR9Ac8gxTgc"
      },
      "source": [
        "!unzip /content/Market-1501-v15.09.15.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQGp_vI0QiN_"
      },
      "source": [
        "!rm /content/Market-1501-v15.09.15/bounding_box_train/Thumbs.db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR8ClLTXy4UP"
      },
      "source": [
        "persons = os.listdir('/content/Market-1501-v15.09.15/bounding_box_train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQvjGTCZy-vu"
      },
      "source": [
        "pattern=re.compile(r'([-\\d]+)_c(\\d)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0rCL19lWRUX"
      },
      "source": [
        "count = np.ones(len(persons))\n",
        "for img_name in persons:\n",
        "  id,cam = pattern.search(img_name).groups()\n",
        "  \n",
        "  count[int(id)] += 1  \n",
        "\n",
        "k=0\n",
        "for j in count:\n",
        "  if(j<18 and j>15):\n",
        "    k +=1\n",
        "\n",
        "print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL0b1FmZy-x4"
      },
      "source": [
        "current_path = '/content/Market-1501-v15.09.15/bounding_box_train'\n",
        "for person in tqdm(persons):\n",
        "  if os.path.splitext(person)[1]== '.jpg':\n",
        "    id,cam = pattern.search(person).groups()\n",
        "    if(count[int(id)]<18 and count[int(id)] > 15):\n",
        "      if(True):\n",
        "        if os.path.exists('/content/persons/'+id) == False:\n",
        "          os.makedirs('../content/persons/'+id)\n",
        "        shutil.move(current_path+'/'+person, '../content/persons/'+id+'/'+person)\n",
        "\n",
        "      else:\n",
        "        if os.path.exists('/content/persons_test/'+id+'/'+cam) == False:\n",
        "          os.makedirs('../content/persons_test/'+id+'/'+cam)\n",
        "        shutil.move(current_path+'/'+person, '../content/persons_test/'+id+'/'+cam+'/'+person)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvMfuheCy-0U"
      },
      "source": [
        "persons_imgs = []\n",
        "for subdir, dirs, files in os.walk('/content/persons'):\n",
        "\n",
        "  for file in files:\n",
        "    filepath = subdir + os.sep + file\n",
        "    if filepath.endswith(\".jpg\"):\n",
        "      persons_imgs.append(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCxiaN83y-3S"
      },
      "source": [
        "for i in range(20):\n",
        "  selected = random.choice(persons_imgs)\n",
        "  selected_img = selected.split('/')\n",
        "  selected_img = selected_img[-1]\n",
        "  id,cam = pattern.search(selected_img).groups()\n",
        "  if os.path.exists('/content/persons_test') == False:\n",
        "      os.makedirs('../content/persons_test')\n",
        "  shutil.move(selected, '../content/persons_test/'+selected_img)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUspOMKx2qts"
      },
      "source": [
        "root_dir = '../content/persons/'\n",
        "categories = [[folder, os.listdir(root_dir + folder)] for folder in os.listdir(root_dir)]\n",
        "print(len(categories))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPu3W5AYWIB6"
      },
      "source": [
        "class Person_reid(Dataset):\n",
        "    def __init__(self, categories, root_dir, setSize, transform=None):\n",
        "        self.categories = categories\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.setSize = setSize\n",
        "    def __len__(self):\n",
        "        return self.setSize\n",
        "    def __getitem__(self, idx):\n",
        "        img1 = None\n",
        "        img2 = None\n",
        "        label = None\n",
        "        if idx % 2 == 0:\n",
        "            category = random.choice(categories)\n",
        "            imgDir = root_dir + category[0]\n",
        "            img1Name = random.choice(os.listdir(imgDir))\n",
        "            img2Name = random.choice(os.listdir(imgDir))\n",
        "            img1 = Image.open(imgDir + '/' + img1Name)\n",
        "            img2 = Image.open(imgDir + '/' + img2Name)\n",
        "            label = 1.0\n",
        "        else:\n",
        "            category1, category2 = random.choice(categories), random.choice(categories)\n",
        "            category1, category2 = random.choice(categories), random.choice(categories)\n",
        "\n",
        "            imgDir1, imgDir2 = root_dir + category1[0], root_dir + category2[0]\n",
        "            img1Name = random.choice(os.listdir(imgDir1))\n",
        "            img2Name = random.choice(os.listdir(imgDir2))\n",
        "            while img1Name == img2Name:\n",
        "                img2Name = random.choice(os.listdir(imgDir2))\n",
        "            label = 0.0\n",
        "            img1 = Image.open(imgDir1 + '/' + img1Name)\n",
        "            img2 = Image.open(imgDir2 + '/' + img2Name)\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "        return img1, img2, torch.from_numpy(np.array([label], dtype=np.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXbKaziZYHm5"
      },
      "source": [
        "dataSize = 12000 # self-defined dataset size\n",
        "TRAIN_PCT = 0.8 # percentage of entire dataset for training\n",
        "train_size = int(dataSize * TRAIN_PCT)\n",
        "val_size = dataSize - train_size\n",
        "\n",
        "transformations = transforms.Compose(\n",
        "    [\n",
        "     transforms.ToTensor()\n",
        "    ]) \n",
        "\n",
        "market_id_dataset = Person_reid(categories, root_dir, dataSize, transformations)\n",
        "train_set, val_set = random_split(market_id_dataset, [train_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFYZwQkMkUEF"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 64, 3) \n",
        "        self.conv2 = nn.Conv2d(64, 128, 2)  \n",
        "        self.conv3 = nn.Conv2d(128, 128, 2)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(256*14*6, 4096)\n",
        "        self.fcOut = nn.Linear(4096, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    \n",
        "    def convs(self, x):\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.convs(x1)\n",
        "        x1 = x1.view(x1.shape[0],-1)\n",
        "        x1 = self.sigmoid(self.fc1(x1))\n",
        "\n",
        "        \n",
        "        x2 = self.convs(x2)\n",
        "        x2 = x2.view(x1.shape[0],-1)\n",
        "        x2 = self.sigmoid(self.fc1(x2))\n",
        "\n",
        "        x = torch.abs(x1 - x2)\n",
        "        x = self.fcOut(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bfrygonk0X8"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "siameseBaseLine = Net()\n",
        "siameseBaseLine = siameseBaseLine.to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model architecture:\\n\\n', model.parameters)\n",
        "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
        "    \n",
        "count_parameters(siameseBaseLine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Tjbr2U5K5P"
      },
      "source": [
        "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
        "    if save_path==None:\n",
        "        return\n",
        "    save_path = save_path \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'val_loss': val_loss}\n",
        "\n",
        "    torch.save(state_dict, save_path)\n",
        "\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "    save_path = f'siameseNet-batchnorm50.pt'\n",
        "    state_dict = torch.load(save_path)\n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    val_loss = state_dict['val_loss']\n",
        "    print(f'Model loaded from <== {save_path}')\n",
        "    \n",
        "    return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j9dq7Ns5YQY"
      },
      "source": [
        "def train(model, train_loader, val_loader, num_epochs, criterion, save_name):\n",
        "    best_val_loss = float(\"Inf\") \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    cur_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        print(\"Starting epoch \" + str(epoch+1))\n",
        "\n",
        "        for img1, img2, labels in train_loader:\n",
        "            # Forward\n",
        "            img1 = img1.to(device)\n",
        "            img2 = img2.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(img1, img2)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for img1, img2, labels in val_loader:\n",
        "                img1 = img1.to(device)\n",
        "                img2 = img2.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(img1, img2)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
        "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
        "    \n",
        "    print(\"Finished Training\")  \n",
        "    return train_losses, val_losses  \n",
        "\n",
        "# evaluation metrics\n",
        "def eval(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        print('Starting Iteration')\n",
        "        count = 0\n",
        "        for mainImg, imgSets, label in test_loader:\n",
        "            mainImg = mainImg.to(device)\n",
        "            predVal = 0\n",
        "            pred = -1\n",
        "            for i, testImg in enumerate(imgSets):\n",
        "                testImg = testImg.to(device)\n",
        "                output = model(mainImg, testImg)\n",
        "                if output > predVal:\n",
        "                    pred = i\n",
        "                    predVal = output\n",
        "            label = label.to(device)\n",
        "            if pred == label:\n",
        "                correct += 1\n",
        "            count += 1\n",
        "            if count % 20 == 0:\n",
        "                print(\"Current Count is: {}\".format(count))\n",
        "                print('Accuracy on n way: {}'.format(correct/count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PftaxVojc8MK"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.001)\n",
        "num_epochs = 80\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "save_path = 'siameseNet.pt'\n",
        "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq6QcW9H825l"
      },
      "source": [
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlJN1ILoJVAG"
      },
      "source": [
        "trans = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYbBzH05iAid"
      },
      "source": [
        "train_persons = []\n",
        "for subdir, dirs, files in os.walk('/content/persons'):\n",
        "    for file in files:\n",
        "        filepath = subdir + os.sep + file\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            train_persons.append(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQzmGyTygRdx"
      },
      "source": [
        "len(train_persons)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJg1xdM5exjA"
      },
      "source": [
        "query_path = '/content/persons_test/0482_c1s2_053121_01.jpg'\n",
        "query = Image.open(query_path)\n",
        "query = trans(query)\n",
        "query = query.unsqueeze(0)\n",
        "query = query.to(device)\n",
        "\n",
        "scores = []\n",
        "ids = []\n",
        "\n",
        "for person in train_persons:\n",
        "  img = person.split('/')\n",
        "  img = img[-1]\n",
        "  id,cam = pattern.search(img).groups()\n",
        "  match = Image.open(person)\n",
        "  match = trans(match)\n",
        "\n",
        "  match = match.unsqueeze(0)\n",
        "  match = match.to(device)\n",
        "\n",
        "  outputs = siameseBaseLine(query, match)\n",
        "  out = outputs.cpu().detach().numpy()\n",
        "  \n",
        "  ids.append(person)\n",
        "  scores.append(out[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUU8_1m-p60I"
      },
      "source": [
        "img1 = cv2.imread(query_path)\n",
        "img2 = cv2.imread(ids[scores.index(max(scores))])\n",
        "\n",
        "cv2_imshow(img1)\n",
        "cv2_imshow(img2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zzAOQrGKUZ1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}